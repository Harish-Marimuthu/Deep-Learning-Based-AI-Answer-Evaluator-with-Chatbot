# -*- coding: utf-8 -*-
"""Capstone_Project_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rMCQofOS3Smg41eWUnUlcWmmsCohd7Ri
"""



import pandas as pd
import numpy as np
import re
import nltk
import string
import torch
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import BertTokenizer, BertModel
import pandas as pd
import numpy as np
import re
import nltk
import string
import spacy
import torch
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import BertTokenizer, BertModel
from sentence_transformers import SentenceTransformer
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, LeakyReLU
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.regularizers import l2

pip install spacy

# Download necessary resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

df = pd.read_csv("/content/Wikipedia - QA-train.csv")

df

# Select only the required 3 columns from the original df
train_df = df[['Question', 'Sentence', 'Label']].copy()  # Explicitly create a copy
train_df

# To check missing values

train_df.isnull().sum()

# To check duplicated values

train_df.duplicated().sum()

train_df.drop_duplicates(inplace=True)

# To check duplicated values

train_df.duplicated().sum()

nlp = spacy.load("en_core_web_sm")

def clean_text(text):
    if pd.isna(text):  # Handle missing values
        return ""
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

train_df['clean_question'] = train_df['Question'].apply(clean_text)
train_df['clean_sentence'] = train_df['Sentence'].apply(clean_text)

"""Tokenization"""

def tokenize_texts(texts):
    return [[token.text for token in doc if token.is_alpha] for doc in nlp.pipe(texts, batch_size=50)]

train_df['tokenized_question'] = tokenize_texts(train_df['clean_question'])
train_df['tokenized_sentence'] = tokenize_texts(train_df['clean_sentence'])

"""Stopword Removal"""

def remove_stopwords(tokens):
    return [word for word in tokens if not nlp.vocab[word].is_stop]

train_df['no_stopwords_question'] = train_df['tokenized_question'].apply(remove_stopwords)
train_df['no_stopwords_sentence'] = train_df['tokenized_sentence'].apply(remove_stopwords)

"""Lemmatization"""

import spacy

nlp = spacy.load("en_core_web_sm")

# Convert lists of tokens to strings before processing
def lemmatize_texts(texts):
    texts = [" ".join(tokens) for tokens in texts]  # Convert list of tokens to a string
    return [[token.lemma_ for token in doc] for doc in nlp.pipe(texts, batch_size=50)]

# Apply efficiently to the entire column
train_df['lemmatized_question'] = lemmatize_texts(train_df['no_stopwords_question'])
train_df['lemmatized_sentence'] = lemmatize_texts(train_df['no_stopwords_sentence'])

"""SBERT"""

from sentence_transformers import SentenceTransformer

# Load SBERT model
sbert_model = SentenceTransformer("all-MiniLM-L6-v2")

# Ensure lemmatized text is converted to a string
train_df["lemmatized_question"] = train_df["lemmatized_question"].apply(lambda x: " ".join(x) if isinstance(x, list) else str(x))
train_df["lemmatized_sentence"] = train_df["lemmatized_sentence"].apply(lambda x: " ".join(x) if isinstance(x, list) else str(x))

# Encode efficiently using batch processing
train_df["question_embedding"] = list(sbert_model.encode(train_df["lemmatized_question"].tolist(), batch_size=32, convert_to_numpy=True))
train_df["sentence_embedding"] = list(sbert_model.encode(train_df["lemmatized_sentence"].tolist(), batch_size=32, convert_to_numpy=True))

# 2

from sentence_transformers import SentenceTransformer

# Load the best model for accuracy
sbert_model = SentenceTransformer("all-mpnet-base-v2")

# Ensure lemmatized text is in string format
train_df["lemmatized_question"] = train_df["lemmatized_question"].astype(str)
train_df["lemmatized_sentence"] = train_df["lemmatized_sentence"].astype(str)

# Encode efficiently using batch processing
train_df["question_embedding"] = list(sbert_model.encode(
    train_df["lemmatized_question"].tolist(), batch_size=32, convert_to_numpy=True)
)
train_df["sentence_embedding"] = list(sbert_model.encode(
    train_df["lemmatized_sentence"].tolist(), batch_size=32, convert_to_numpy=True)
)

"""class wise"""

import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, LeakyReLU, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.initializers import HeNormal
from tensorflow.keras import backend as K
from sklearn.utils.class_weight import compute_class_weight

# Convert embeddings to numpy arrays
question_embedding = np.stack(train_df["question_embedding"].values)
sentence_embedding = np.stack(train_df["sentence_embedding"].values)

# Convert NumPy arrays to PyTorch tensors
question_embedding_torch = torch.tensor(question_embedding, dtype=torch.float32)
sentence_embedding_torch = torch.tensor(sentence_embedding, dtype=torch.float32)

# Concatenate embeddings along dim=1
X = torch.cat((question_embedding_torch, sentence_embedding_torch), dim=1)

# Convert labels to tensor
y = torch.tensor(train_df['Label'].values, dtype=torch.float32)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2, random_state=42)

# Ensure X_train is 2D before SMOTE
X_train_2d = X_train.reshape(X_train.shape[0], -1)

# Apply SMOTE with a higher sampling ratio
smote = SMOTE(sampling_strategy=0.9, random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_2d, y_train)

# Convert y_train_resampled to float32
y_train_resampled = np.array(y_train_resampled, dtype=np.float32)

# Reshape for LSTM
time_steps = 2
X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], time_steps, -1))
X_test = X_test.reshape((X_test.shape[0], time_steps, -1))

print("Shape after SMOTE:", X_train_resampled.shape, y_train_resampled.shape)

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)
class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}
print("Class Weights:", class_weight_dict)

# Define Focal Loss function
def focal_loss(alpha=0.25, gamma=2.0):
    def loss(y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)
        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)
        return -K.mean(alpha * K.pow(1.0 - pt, gamma) * K.log(pt))
    return loss

# Define LSTM Model
model = Sequential([
    Input(shape=(time_steps, X_train_resampled.shape[2])),

    LSTM(300, return_sequences=False),
    BatchNormalization(),
    Dropout(0.2),  # Reduced dropout

    Dense(128, kernel_initializer=HeNormal()),  # Increased neurons
    BatchNormalization(),
    LeakyReLU(alpha=0.1),

    Dense(64, kernel_initializer=HeNormal()),
    BatchNormalization(),
    LeakyReLU(alpha=0.1),

    Dense(1, activation='sigmoid')
])

# Compile Model
model.compile(
    loss=focal_loss(),  # Using focal loss
    optimizer=Adam(learning_rate=0.0001),
    metrics=['accuracy']
)

# Train the model with class weights
history = model.fit(
    X_train_resampled, y_train_resampled,
    epochs=15, batch_size=32,  # Increased epochs
    validation_data=(X_test, y_test),
    class_weight=class_weight_dict  # Apply class weights
)

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

import pickle

# Assuming 'model' is your trained LSTM model
with open("lstm_model.pkl", "wb") as f:
    pickle.dump(model, f)

print("LSTM Model saved as 'lstm_model.pkl'")

model.save("model.h5")

model.save("model.keras")



"""Validation"""

validation_df = pd.read_csv("/content/Wikipedia - QA-validation.csv")

validation_df

validation_df.isnull().sum()

validation_df.duplicated().sum()

# Select only the required 3 columns from the original df
validation_df_1 = validation_df[['Question', 'Sentence', 'Label']].copy()  # Explicitly create a copy
validation_df_1

# Apply text cleaning function
validation_df_1['clean_question'] = validation_df_1['Question'].apply(clean_text)
validation_df_1['clean_sentence'] = validation_df_1['Sentence'].apply(clean_text)

# Tokenization
validation_df_1['tokenized_question'] = validation_df_1['clean_question'].apply(tokenize_text)
validation_df_1['tokenized_sentence'] = validation_df_1['clean_sentence'].apply(tokenize_text)

# Stopword Removal
validation_df_1['no_stopwords_question'] = validation_df_1['tokenized_question'].apply(remove_stopwords)
validation_df_1['no_stopwords_sentence'] = validation_df_1['tokenized_sentence'].apply(remove_stopwords)

# Lemmatization
validation_df_1['lemmatized_question'] = lemmatize_texts(validation_df_1['no_stopwords_question'])
validation_df_1['lemmatized_sentence'] = lemmatize_texts(validation_df_1['no_stopwords_sentence'])

# Convert to string before embedding
validation_df_1["lemmatized_question"] = validation_df_1["lemmatized_question"].apply(lambda x: " ".join(x))
validation_df_1["lemmatized_sentence"] = validation_df_1["lemmatized_sentence"].apply(lambda x: " ".join(x))

# Encode using SBERT
validation_df_1["question_embedding"] = list(sbert_model.encode(validation_df_1["lemmatized_question"].tolist(), batch_size=32, convert_to_numpy=True))
validation_df_1["sentence_embedding"] = list(sbert_model.encode(validation_df_1["lemmatized_sentence"].tolist(), batch_size=32, convert_to_numpy=True))

# Convert embeddings to NumPy arrays
question_embedding_val = np.stack(validation_df_1["question_embedding"].values)
sentence_embedding_val = np.stack(validation_df_1["sentence_embedding"].values)

# Convert to PyTorch tensors
question_embedding_torch_val = torch.tensor(question_embedding_val, dtype=torch.float32)
sentence_embedding_torch_val = torch.tensor(sentence_embedding_val, dtype=torch.float32)

# Concatenate embeddings
X_val = torch.cat((question_embedding_torch_val, sentence_embedding_torch_val), dim=1).numpy()

# Get labels
y_val = validation_df_1['Label'].values.astype(np.float32)

from imblearn.over_sampling import SMOTE

# Apply SMOTE on validation set
smote = SMOTE(sampling_strategy=0.7, random_state=42)
X_val_resampled, y_val_resampled = smote.fit_resample(X_val.reshape(X_val.shape[0], -1), y_val)

# Reshape back for LSTM
time_steps = 2
X_val_resampled = X_val_resampled.reshape((X_val_resampled.shape[0], time_steps, -1))

# Evaluate again
loss, accuracy = model.evaluate(X_val_resampled, y_val_resampled)
print(f'Validation Loss: {loss:.4f}')
print(f'Validation Accuracy: {accuracy:.4f}')

# Get predictions
y_pred_probs = model.predict(X_val)
y_pred = (y_pred_probs > 0.3).astype(int)  # Convert probabilities to binary labels

# Compare with actual labels
from sklearn.metrics import classification_report, confusion_matrix

print("Confusion Matrix:")
print(confusion_matrix(y_val, y_pred))

print("\nClassification Report:")
print(classification_report(y_val, y_pred))





"""Testing"""

test_df = pd.read_csv("/content/Wikipedia - QA-test.csv")
test_df

test_df.isnull().sum()

test_df.duplicated().sum()

# Select only the required 3 columns from the original df
test_df_1 = test_df[['Question', 'Sentence', 'Label']].copy()  # Explicitly create a copy
test_df_1

import spacy
import pandas as pd
import re
from sentence_transformers import SentenceTransformer

# Function to clean text (Lowercase, Remove Numbers & Punctuation)
def clean_text(text):
    if pd.isna(text):  # Handle missing values
        return ""
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Apply cleaning
test_df_1['clean_question'] = test_df_1['Question'].astype(str).apply(clean_text)
test_df_1['clean_sentence'] = test_df_1['Sentence'].astype(str).apply(clean_text)

# Function for tokenization (Keep only alphabetic words)
def tokenize_text(text):
    doc = nlp(text)
    return [token.text for token in doc if token.is_alpha]

# Apply tokenization
test_df_1['tokenized_question'] = test_df_1['clean_question'].apply(tokenize_text)
test_df_1['tokenized_sentence'] = test_df_1['clean_sentence'].apply(tokenize_text)

# Function to remove stopwords
def remove_stopwords(tokens):
    return [word for word in tokens if not nlp.vocab[word].is_stop]

# Apply stopword removal
test_df_1['no_stopwords_question'] = test_df_1['tokenized_question'].apply(remove_stopwords)
test_df_1['no_stopwords_sentence'] = test_df_1['tokenized_sentence'].apply(remove_stopwords)

import spacy

# Load spaCy model (optimized for speed)
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

# Function for efficient lemmatization
def lemmatize_text(text):
    if isinstance(text, list):  # Convert list to string
        text = " ".join(text)
    return " ".join([token.lemma_ for token in nlp(text)])  # Process full sentence

# Apply optimized lemmatization
test_df_1['lemmatized_question'] = test_df_1['no_stopwords_question'].apply(lemmatize_text)
test_df_1['lemmatized_sentence'] = test_df_1['no_stopwords_sentence'].apply(lemmatize_text)

# Load SBERT model
sbert_model = SentenceTransformer("all-MiniLM-L6-v2")

# Convert lists of words to sentences
test_df_1["lemmatized_question"] = test_df_1["lemmatized_question"].apply(lambda x: " ".join(x))
test_df_1["lemmatized_sentence"] = test_df_1["lemmatized_sentence"].apply(lambda x: " ".join(x))

# Generate embeddings in batch (MUCH FASTER)
test_df_1["question_embedding"] = list(sbert_model.encode(test_df_1["lemmatized_question"].tolist(), show_progress_bar=True))
test_df_1["sentence_embedding"] = list(sbert_model.encode(test_df_1["lemmatized_sentence"].tolist(), show_progress_bar=True))

print("Embeddings generated successfully! 🚀")

# Convert embeddings to NumPy arrays
test_question_embeddings = np.vstack(test_df_1["question_embedding"].values)
test_sentence_embeddings = np.vstack(test_df_1["sentence_embedding"].values)

# Concatenate embeddings for final input
test_X = np.hstack((test_question_embeddings, test_sentence_embeddings))

test_X = np.array(test_X)  # Convert to NumPy array
test_X = test_X.reshape((test_X.shape[0], 2, -1))  # Adjust based on training input shape


# Extract labels
test_y = test_df_1['Label'].values.astype(np.float32)

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(test_X, test_y)

# Print results
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

from sklearn.metrics import precision_recall_curve
import numpy as np

# Get model predictions
y_pred_probs = model.predict(X_test)

# Calculate Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)

# Find best threshold (based on highest F1-score)
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print("Best threshold:", best_threshold)

# Convert probabilities to binary labels
y_pred_labels = (y_pred_probs > best_threshold).astype(int)

from sklearn.metrics import precision_recall_curve
import numpy as np

# Get model predictions
y_pred_probs = model.predict(X_test)

# Ensure y_pred_probs matches y_test in length
y_pred_probs = y_pred_probs[:len(y_test)]

# Calculate Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)

# Ensure thresholds array is not empty
if thresholds.size > 0:
    # Find best threshold (based on highest F1-score)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero
    best_threshold = thresholds[np.argmax(f1_scores)]
else:
    best_threshold = 0.5  # Default threshold if no valid thresholds found

print("Best threshold:", best_threshold)

# Convert probabilities to binary labels
y_pred_labels = (y_pred_probs > best_threshold).astype(int)

y_pred_labels = (y_pred_probs > best_threshold).astype(int)

from sklearn.metrics import classification_report, confusion_matrix

print("Classification Report:\n", classification_report(y_test, y_pred_labels))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_labels))

model.save("accuracy_evaluator.h5")



"""Flask api"""

!pip install rouge-score

from flask import Flask, request, jsonify
import tensorflow as tf
from tensorflow.keras.models import load_model
from transformers import BertTokenizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer

import numpy as np

app = Flask(__name__)

from tensorflow.keras.models import load_model
model = load_model("accuracy_evaluator.h5")


# Load Tokenizer (If Using BERT)
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# NLP Scoring Functions
def compute_cosine_similarity(embedding1, embedding2):
    return cosine_similarity([embedding1], [embedding2])[0][0]

def compute_bleu(reference, candidate):
    return sentence_bleu([reference.split()], candidate.split())

def compute_rouge(reference, candidate):
    rouge = Rouge()
    scores = rouge.get_scores(candidate, reference)
    return scores[0]['rouge-l']['f']

# Model Prediction Function
def evaluate_answer(user_question, ai_answer, expert_answer):
    # Convert text to embeddings (Example: Using BERT)
    user_input = tokenizer(user_question, return_tensors="tf", padding=True, truncation=True)["input_ids"]
    ai_input = tokenizer(ai_answer, return_tensors="tf", padding=True, truncation=True)["input_ids"]
    expert_input = tokenizer(expert_answer, return_tensors="tf", padding=True, truncation=True)["input_ids"]

    # Get model prediction (accuracy score between 0-1)
    prediction = model.predict([user_input, ai_input, expert_input])[0][0]

    # Compute Similarity Scores
    cosine_sim = compute_cosine_similarity(user_input.numpy().flatten(), expert_input.numpy().flatten())
    bleu_score = compute_bleu(expert_answer, ai_answer)
    rouge_score = compute_rouge(expert_answer, ai_answer)

    # Final Accuracy Score (weighted average)
    final_score = (prediction * 0.5) + (cosine_sim * 0.2) + (bleu_score * 0.2) + (rouge_score * 0.1)

    return round(final_score, 4)

# API Route: Evaluate Answer
@app.route("/evaluate", methods=["POST"])
def evaluate():
    data = request.json
    user_question = data.get("question")
    ai_answer = data.get("ai_answer")
    expert_answer = data.get("expert_answer")

    if not user_question or not ai_answer or not expert_answer:
        return jsonify({"error": "Missing input data"}), 400

    accuracy_score = evaluate_answer(user_question, ai_answer, expert_answer)

    return jsonify({"accuracy_score": accuracy_score})

# Run the API
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)

l=[1,1,2,2,3,3,1,1,1]

positions=[]
for i in range(len(l)):
  if l[i]==1:
    positions.append(i)
print(positions)

